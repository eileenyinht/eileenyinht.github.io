---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a **Ph.D. candidate in Computer Science** at the [New Jersey Institute of Technology (NJIT)](https://www.njit.edu/), supervised by [Prof. Przemyslaw Musialski](https://people.njit.edu/faculty/przemyslaw.musialski).  
My research focuses on **3D reconstruction**, **generative AI**, and **neural surface representation**.  
In particular, I develop **implicit neural networks** for 3D shape reconstruction and representation, bridging geometry processing and machine learning.

Before joining NJIT, I earned **dual Bachelor’s degrees** from *Jilin University* —  
a **B.Sc. in Information and Computational Science** (College of Mathematics)  
and a **B.Eng. in Computer Application** (College of Computer Science).  

---

<span style="color:red; font-weight:bold;">I am currently seeking job opportunities — please feel free to reach out!</span>

Publications
======
## Research Highlights

**Ph.D. Student — Graphics and Neural Surface Representation**  
_New Jersey Institute of Technology (NJIT), 09/2021–Present_

### [SIGGRAPH Asia 2025]  
**A Finite Difference Approximation of Second-Order Regularization for Neural-SDFs**  
Developed a lightweight optimization framework replacing computationally expensive second-order derivatives with finite-difference operators, improving model efficiency and numerical stability.  
Enabled large-scale neural modeling using only first-order gradients — an approach generalizable to AIGC and reinforcement learning systems requiring interpretable and efficient optimization.

### [Pacific Graphics 2025]  
**FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models**  
Proposed an off-diagonal loss design that stabilizes model training and doubles convergence speed.  
Demonstrated the framework’s adaptability across domains — from 3D surface learning to general deep network regularization and AI optimization under limited compute budgets.

### [ISVC 2025]  
**Scheduling Curvature-Aware Loss with Annealed Weights**  
Designed an adaptive scheduling strategy that dynamically adjusts regularization weights during training to maintain convergence stability.  
Improved overall model accuracy by **35%**, providing a general mechanism for balanced loss design in large-scale AI and multi-agent learning scenarios.
